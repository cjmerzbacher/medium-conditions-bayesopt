{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5268353-3435-44c7-88e5-a3b1326b5e44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multi-objective Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ed588-3595-4c5a-b344-c1e2d36fb6b3",
   "metadata": {
    "id": "374ed588-3595-4c5a-b344-c1e2d36fb6b3"
   },
   "source": [
    "https://botorch.org/docs/multi_objective\n",
    "* \"The goal in MOBO is learn the Pareto front: the set of optimal trade-offs, where an improvement in one objective means deteriorating another objective.\"\n",
    "* \"The MC-based acquisition functions support using the sample average approximation for rapid convergence\"\n",
    "* Additionally, variations on ParEGO can be trivially implemented using an augmented Chebyshev scalarization as the objective with an EI-type single-objective acquisition function such as qLogNoisyExpectedImprovement. The get_chebyshev_scalarization convenience function generates these scalarizations.\n",
    "  \n",
    "https://botorch.org/tutorials/constrained_multi_objective_bo\n",
    "Considerations\n",
    "* BoTorch assumes maximization\n",
    "  * leave growth rate as it is\n",
    "  * maximise negative of costs\n",
    "\n",
    "\n",
    "from: https://botorch.org/tutorials/constrained_multi_objective_bo\n",
    "\n",
    "\"For batch optimization (or in noisy settings), we strongly recommend using NEHVI rather than EHVI [1] because it is far more efficient than EHVI and mathematically equivalent in the noiseless setting.\" <- but that's hypervolumen...\n",
    "\n",
    "\"Note: EHVI aggressively exploits parallel hardware and is much faster when run on a GPU. See [1] for details.\"\n",
    "-> use GPU and run on server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c05ac1-0a6f-443c-a850-86c76567aa90",
   "metadata": {
    "id": "c1c05ac1-0a6f-443c-a850-86c76567aa90",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Function\n",
    "* The model consists of a list of mono-objective models.\n",
    "The likelihood at each point is the sum of all (both) GPâ€™s likelihood.\n",
    "* Acquisition optimisation\n",
    "  * optimises multiple (2) objectives at once\n",
    "  * approach used: qNParEGO\n",
    "    * starts with scalarisation - combine all objectives into a single compound function (augmented chebyshev)\n",
    "  * random process; possibly slower than qEHVI or qNEHVI\n",
    "  *\n",
    "### Concept\n",
    "Within the function\n",
    "1. Change the medium composition using BayesOpt\n",
    "2. For each composition\n",
    "     1. calculate the cost\n",
    "     2. find optimal growth rate using FBA\n",
    "     3. calculate growth/cost (should be maximised)\n",
    "        * How to prevent that the cost is driven to 0? -> even if it drives growth to 0, it will probably be numerically optimal\n",
    "3. Return optimal medium composition, growth rate, costs and relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3070e7b-2268-4be5-b3fc-b197a0669dcd",
   "metadata": {
    "id": "e3070e7b-2268-4be5-b3fc-b197a0669dcd"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ArISd1WjiHjr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArISd1WjiHjr",
    "outputId": "eb84dba5-2329-4257-9d4b-5376609c6f7c"
   },
   "outputs": [],
   "source": [
    "# When running in google colab\n",
    "#pip install cobra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "SNh5PxaHiYGd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNh5PxaHiYGd",
    "outputId": "c2e1896e-0f3a-43c7-9e01-6f78fbe60235"
   },
   "outputs": [],
   "source": [
    "# When running in google colab\n",
    "#pip install botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb93be1-e79c-469d-a533-3820e3e0b15e",
   "metadata": {
    "id": "5eb93be1-e79c-469d-a533-3820e3e0b15e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "# sampler\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "# Acquisition function\n",
    "from botorch.optim.optimize import optimize_acqf_list # for qPAREGO\n",
    "from botorch.optim.optimize import optimize_acqf # for qEHVI and qNEHVI\n",
    "\n",
    "from botorch.acquisition.logei import qLogExpectedImprovement # qPAREGO, assumes noiselessness\n",
    "# qEHVI, assumes noiselessness\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qExpectedHypervolumeImprovement \n",
    "from botorch.acquisition.multi_objective.logei import qLogExpectedHypervolumeImprovement\n",
    "# more efficient for batches than qEHVI, math. equivalent in noiseless setting\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qNoisyExpectedHypervolumeImprovement \n",
    "from botorch.acquisition.multi_objective.logei import qLogNoisyExpectedHypervolumeImprovement\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import FastNondominatedPartitioning\n",
    "\n",
    "from botorch.utils.transforms import unnormalize, normalize # for normalising media components\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated # pareto\n",
    "\n",
    "import random # for initial data\n",
    "import json # to save results\n",
    "import copy # to be able to do deep copies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ab0f7-96ad-4c14-a098-54d20514c50e",
   "metadata": {
    "id": "d64ab0f7-96ad-4c14-a098-54d20514c50e"
   },
   "source": [
    "### Helper Functions & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec471a-b284-4ed5-a01e-b24221e85feb",
   "metadata": {
    "id": "99ec471a-b284-4ed5-a01e-b24221e85feb"
   },
   "outputs": [],
   "source": [
    "# Helper functions to be used across notebooks\n",
    "%run HelperFunctions_MOBO.ipynb\n",
    "\n",
    "# same but for .py version\n",
    "#from HelperFunctions_MOBO import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f51bc6-cf50-49d7-9b66-a4ee10c72bb6",
   "metadata": {},
   "source": [
    "## BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6f688-0470-4932-99ab-84e8eec9726e",
   "metadata": {
    "id": "50c6f688-0470-4932-99ab-84e8eec9726e"
   },
   "outputs": [],
   "source": [
    "# called with normalised growth, cost and production\n",
    "def find_next_candidates(\n",
    "        medium_tensors_normalised_stacked, \n",
    "        growth_tensors,\n",
    "        bounds_tensors_stacked, \n",
    "        n_candidates = 5,\n",
    "        opt_objective = \"growth-cost\",\n",
    "        cost_tensors = None, \n",
    "        production_tensors = None,\n",
    "        AF_type = 'qPAREGO'\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Finds the next medium composition for which to evaluate cost and optimal growth rate\n",
    "    * normalises the medium composition between 0,1\n",
    "    * initialises botorch model (list of SingleTaskGP) and mll\n",
    "    * calculates posterior mean\n",
    "    * uses SobolQMCNormalSampler to sample from posterior\n",
    "    * gives choice of methods\n",
    "    * qPAREGO\n",
    "        * uses qLogExpectedImprovement acquisition function\n",
    "        * uses chebyshev_scalarization to create a vector representation of the two objectives (cost mini, growth rate max)\n",
    "    * qEHVI\n",
    "        * uses qLogExpectedHypervolumeImprovement\n",
    "    * qNEHVI\n",
    "        * uses qNoisyExpectedHypervolumeImprovement\n",
    "    \n",
    "    PARAMETERS\n",
    "    * medium_tensors_normalised_stacked - tensor - all medium compositions previously evaluated, stored as tensors (in order)\n",
    "    * growth_tensors - tensor - corresponding growth rates\n",
    "    * bounds_tensors_stacked - tensors - upper and lower bounds for the values the medium components are allowed to take,\n",
    "    determines the search space;\n",
    "    * n_candidates - integer - how many candidates to find at once = batch_size\n",
    "    * opt_objective - string - the multi-objective\n",
    "    * cost_tensors - tensor - corresponding medium costs\n",
    "    * production_tensors - tensor - corresponding production rates\n",
    "    * AF_type - string - which acquisition function to use\n",
    "\n",
    "    RETURNS\n",
    "    * candidates[0] - tensor - a tensors with the normalised (0,1) medium composition\n",
    "    \"\"\"\n",
    "    # assert that the objective is one of the possibilities\n",
    "    opt_objective_types = ['growth-cost', 'growth-production', 'growth-production-cost',]\n",
    "    if opt_objective not in opt_objective_types:\n",
    "        raise ValueError(\"Invalid objective. Expected one of: %o\" % opt_objective_types)\n",
    "    \n",
    "    '''parameters and conversion to tensors; normalisation of medium composition to (0,1)'''\n",
    "    MC_SAMPLES = 256 # Number of Monte Carlo samples in SobolQMCNormalSampler\n",
    "    n_components = medium_tensors_normalised_stacked.size()[1] # of medium components\n",
    "    # bounds to normalise medium components (x) to\n",
    "    standard_bounds = standard_bounds = torch.tensor([[0.0] * n_components, \n",
    "                                                      [1.0] * n_components]).to(**tkwargs) # normalised bounds for medium composition\n",
    "    # large values -> slower but possibly better accuracy\n",
    "    NUM_RESTARTS =  10 #10 # Number of restarts for acquisition function optimisation\n",
    "    RAW_SAMPLES = 1024 # 1024 # Number of raw samples for initialisation of acquisition optimisation\n",
    "\n",
    "\n",
    "    '''Setup'''\n",
    "    # Set up a Sobol quasi-Monte Carlo sampler for sampling from the posterior\n",
    "    # The sample_shape should correspond to the shape of the posterior samples needed\n",
    "    sampler = SobolQMCNormalSampler(sample_shape = torch.Size([MC_SAMPLES]), seed = MC_SAMPLES)\n",
    "    \n",
    "    # initialise GP model and marginal likelihood (mll)\n",
    "\n",
    "    mll, model = initialise_model(\n",
    "        medium_tensors_normalised_stacked, \n",
    "        growth_tensors, \n",
    "        opt_objective = opt_objective,\n",
    "        cost_tensors = cost_tensors, \n",
    "        production_tensors = production_tensors)\n",
    "\n",
    "    # throws min-max error or normalisation errors, when x or y not formatted correctly\n",
    "    fit_gpytorch_mll(mll) # Fit the model using the maximum marginal likelihood\n",
    "    \n",
    "    print(\"INSIDE find_next_candidate\", \n",
    "          \"\\nGrowth:\", growth_tensors,\n",
    "          \"\\nCosts:\", cost_tensors,\n",
    "          \"\\nProduction\", production_tensors, \n",
    "          sep =\"\\n\")\n",
    "    \n",
    "    '''finding the new candidate'''\n",
    "    if AF_type == 'qPAREGO':\n",
    "        # Compute the posterior mean for the given medium_tensors_stacked using the model\n",
    "        with torch.no_grad():\n",
    "            posterior = model.posterior(medium_tensors_normalised_stacked).mean\n",
    "        \n",
    "        acq_fun_list = [] # List to hold acquisition functions for each candidate\n",
    "\n",
    "        if opt_objective == \"growth-cost\":\n",
    "            # Loop to generate acquisition functions for each candidate\n",
    "            for _ in range(n_candidates):\n",
    "                # Sample weights from the simplex for Chebyshev scalarization\n",
    "                weights = sample_simplex(2, **tkwargs).squeeze() # using 2 weights for scalarization (cost and growth)\n",
    "\n",
    "                # Compute the scalarised objective values for all the training points\n",
    "                scalarized_objective_values = (\n",
    "                    weights[0] * growth_tensors + \n",
    "                    weights[1] * cost_tensors)\n",
    "                # Find the best observed scalarized objective value\n",
    "                best_f = scalarized_objective_values.max().item()\n",
    "\n",
    "                # Define objectivenfor AF\n",
    "                objective = GenericMCObjective(\n",
    "                    get_chebyshev_scalarization(weights, posterior)\n",
    "                )\n",
    "                print(\"Best F:\", best_f,\n",
    "                      \"\\nWeights:\", weights,\n",
    "                      \"\\nPosterior:\", posterior,\n",
    "                      \"\\nObjective:\", objective,\n",
    "                      sep = \"\\n\")\n",
    "                # Define the acquisition function using quasi Monte Carlo EI\n",
    "                acq_fun = qLogExpectedImprovement(\n",
    "                    model = model, # List of SingleTastk GP\n",
    "                    best_f = best_f, # best objective value observed so far - replaces X_baseline in Noisy version\n",
    "                    sampler = sampler, # SobolQMCNormalSampler\n",
    "                    objective = objective, # combination of cost and growth - Chebyshev scalarization\n",
    "                )\n",
    "                acq_fun_list.append(acq_fun)\n",
    "\n",
    "        elif opt_objective == \"growth-production\":\n",
    "            # Loop to generate acquisition functions for each candidate\n",
    "            for _ in range(n_candidates):\n",
    "                # Sample weights from the simplex for Chebyshev scalarization\n",
    "                weights = sample_simplex(2, **tkwargs).squeeze() # using 2 weights for scalarization (growth and production)\n",
    "\n",
    "                # Compute the scalarised objective values for all the training points\n",
    "                scalarized_objective_values = (weights[0] * growth_tensors + weights[1] * production_tensors)\n",
    "                # Find the best observed scalarized objective value\n",
    "                best_f = scalarized_objective_values.max().item()\n",
    "\n",
    "                # Define objective for AF\n",
    "                objective = GenericMCObjective(\n",
    "                    get_chebyshev_scalarization(weights, posterior)\n",
    "                )\n",
    "\n",
    "                # Define the acquisition function using quasi Monte Carlo EI\n",
    "                acq_fun = qLogExpectedImprovement(\n",
    "                    model = model, # List of SingleTastk GP\n",
    "                    best_f = best_f, # best objective value observed so far - replaces X_baseline in Noisy version\n",
    "                    sampler = sampler, # SobolQMCNormalSampler\n",
    "                    objective = objective, # combination of growth and production - Chebyshev scalarization\n",
    "                )\n",
    "                acq_fun_list.append(acq_fun)\n",
    "        \n",
    "        elif opt_objective == \"growth-production-cost\":\n",
    "            # Loop to generate acquisition functions for each candidate\n",
    "            for _ in range(n_candidates):\n",
    "                # Sample weights from the simplex for Chebyshev scalarization\n",
    "                weights = sample_simplex(3, **tkwargs).squeeze() # using 3 weights for scalarization (cost, production and growth)\n",
    "\n",
    "                # Compute the scalarised objective values for all the training points\n",
    "                scalarized_objective_values = (\n",
    "                    weights[0] * growth_tensors + \n",
    "                    weights[1] * production_tensors +\n",
    "                    weights[2] * cost_tensors)\n",
    "                # Find the best observed scalarized objective value\n",
    "                best_f = scalarized_objective_values.max().item()\n",
    "\n",
    "                # Define objective for AF\n",
    "                objective = GenericMCObjective(\n",
    "                    get_chebyshev_scalarization(weights, posterior)\n",
    "                )\n",
    "\n",
    "                # Define the acquisition function using quasi Monte Carlo EI\n",
    "                acq_fun = qLogExpectedImprovement(\n",
    "                    model = model, # List of SingleTastk GP\n",
    "                    best_f = best_f, # best objective value observed so far - replaces X_baseline in Noisy version\n",
    "                    sampler = sampler, # SobolQMCNormalSampler\n",
    "                    objective = objective, # combination of cost and growth - Chebyshev scalarization\n",
    "                )\n",
    "                acq_fun_list.append(acq_fun)\n",
    "\n",
    "        candidates, _ = optimize_acqf_list(\n",
    "            acq_function_list = acq_fun_list,  # List of acquisition functions to optimise\n",
    "            bounds = standard_bounds, # The normalised bounds for optimisation\n",
    "            num_restarts = NUM_RESTARTS, # Number of restarts for optimisation\n",
    "            raw_samples = RAW_SAMPLES, # Number of raw samples for initialisation (?)\n",
    "            options = {\"batch_limit\": 10, \"maxiter\": 200,} # Options for acquisition function optimisation\n",
    "        )\n",
    "\n",
    "        return candidates\n",
    "    \n",
    "    elif ((AF_type == 'qEHVI') or (AF_type == 'qNEHVI')):\n",
    "        \"\"\" Reference Point\n",
    "        EHVI requires specifying a reference point, which is the lower bound on the objectives \n",
    "        used for computing hypervolume. \n",
    "        In practice the reference point can be set \n",
    "        1) using domain knowledge to be slightly worse than the lower bound of objective values, \n",
    "        where the lower bound is the minimum acceptable value of interest for each objective, \n",
    "        or \n",
    "        2) using a dynamic reference point selection strategy.\n",
    "        \"\"\"\n",
    "        # TODO: adjust ref_point\n",
    "        # working with normalised y's -  maybe shouldn't? \n",
    "        # or fix ref point and scale it with normalisation bound\n",
    "        if opt_objective == \"growth-cost\":\n",
    "            ref_point = torch.tensor([0.05, -0.95]) # just to see if it runs\n",
    "            # non-normalised\n",
    "            #ref_point = torch.tensor([0.5, -2000])\n",
    "        \n",
    "        elif opt_objective == \"growth-production\":\n",
    "            ref_point = torch.tensor([0.05, 0.05]) # just to see if it runs\n",
    "            # non-normalised\n",
    "            #ref_point = torch.tensor([0.2, 0.001])\n",
    "\n",
    "        elif opt_objective == \"growth-production-cost\":\n",
    "            ref_point = torch.tensor([0.05, 0.05, -0.95]) # just to see if it run\n",
    "            # non-normalised\n",
    "            #ref_point = torch.tensor([0.2, 0.001, -3000])\n",
    "\n",
    "\n",
    "        if AF_type == 'qEHVI':\n",
    "            '''def optimize_qehvi_and_get_observation(model, train_x, train_obj, sampler):'''\n",
    "            # Compute the posterior mean for the given medium_tensors_stacked using the model\n",
    "            with torch.no_grad():\n",
    "                posterior = model.posterior(medium_tensors_normalised_stacked).mean\n",
    "\n",
    "            '''FastNondominatedPartitioning'''\n",
    "            # assumes maximization\n",
    "            # Parameters:\n",
    "            # ref_point (Tensor) â€“ A m-dim tensor containing the reference point.\n",
    "            # Y (Optional[Tensor]) â€“ A (batch_shape) x n x m-dim tensor.        \n",
    "            partitioning = FastNondominatedPartitioning(\n",
    "                ref_point = ref_point, Y = posterior\n",
    "                )\n",
    "\n",
    "            # Define the acquisition function using qEHVI\n",
    "            \"\"\"\n",
    "            acq_func = qExpectedHypervolumeImprovement(\n",
    "                model = model,\n",
    "                ref_point = ref_point,\n",
    "                partitioning = partitioning,\n",
    "                sampler = sampler,\n",
    "                )\n",
    "            \"\"\"\n",
    "            # Try qLogEHVI, as recommended\n",
    "            acq_func = qLogExpectedHypervolumeImprovement(\n",
    "                model = model,\n",
    "                ref_point = ref_point,\n",
    "                partitioning = partitioning,\n",
    "                sampler = sampler,\n",
    "                )\n",
    "        \n",
    "        # q-Log Noisy Expected Hypervolume Improvement supporting m>=2 outcomes.\n",
    "        elif AF_type == 'qNEHVI':\n",
    "            # partition non-dominated space into disjoint rectangles\n",
    "            \"\"\"\n",
    "            acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "                model = model,\n",
    "                ref_point = ref_point.tolist(),  # use known reference point\n",
    "                X_baseline = medium_tensors_normalised_stacked, # normalize(train_x, problem.bounds)\n",
    "                prune_baseline = True,  # prune baseline points that have estimated zero probability of being Pareto optimal\n",
    "                sampler = sampler,\n",
    "            )\n",
    "            \"\"\"\n",
    "            # Try qLogEHVI, as recommended\n",
    "            acq_func = qLogNoisyExpectedHypervolumeImprovement(\n",
    "                model = model,\n",
    "                ref_point = ref_point.tolist(),  # use known reference point\n",
    "                X_baseline = medium_tensors_normalised_stacked, # normalize(train_x, problem.bounds)\n",
    "                prune_baseline = True,  # prune baseline points that have estimated zero probability of being Pareto optimal\n",
    "                sampler = sampler,\n",
    "             )\n",
    "            \n",
    "        # optimize\n",
    "        candidates, _ = optimize_acqf(\n",
    "            acq_function = acq_func, # qEHVI\n",
    "            bounds = standard_bounds, # The normalised bounds for optimisation\n",
    "            q = n_candidates,\n",
    "            num_restarts = NUM_RESTARTS, # Number of restarts for optimisation\n",
    "            raw_samples = RAW_SAMPLES, # used for intialization heuristic\n",
    "            options = {\"batch_limit\": 20, \"maxiter\": 200}, # Options for acquisition function optimisation\n",
    "            sequential = True,\n",
    "            )\n",
    "            \n",
    "        return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e81206-6abd-4ea8-96a8-ecd1fc41e3c3",
   "metadata": {
    "id": "b2e81206-6abd-4ea8-96a8-ecd1fc41e3c3"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebade7a-1f07-4932-a448-5ed3acba40c8",
   "metadata": {
    "id": "3ebade7a-1f07-4932-a448-5ed3acba40c8"
   },
   "outputs": [],
   "source": [
    "def media_BayesOpt(\n",
    "        MetModel, medium = None, bounds = None, costs = None,\n",
    "        opt_objective = \"growth-cost\",\n",
    "        biomass_objective = None, \n",
    "        production_objective = None,\n",
    "        n_start = 20, n_iter = 50, n_candidates = 1,\n",
    "        AF_type = 'qPAREGO',\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Performs medium optimisation for various objectives\n",
    "    * trade-off between growth rate and cost\n",
    "    * trade-off between growth rate and production\n",
    "\n",
    "    1. Sets default values for medium, bounds and costs if not provided by the user\n",
    "    2. Performs optimisation n_iter (default = 50) times\n",
    "        1. calls generate_initial_data(args) to generate initial data points\n",
    "        2. finds new candidate medium calling find_next_candidate(args)\n",
    "        3. evaluates new medium for growth rate and costs\n",
    "        4. keeps all values\n",
    "    3. returns optimal composition alongside corresponding cost, growth, and cost-growth trade-off\n",
    "\n",
    "    PARAMETERS:\n",
    "    * MetModel - COBRApy model - the metabolic model to be evaluated\n",
    "    * medium - dictionary - the medium composition of that model; if not provided defaults to default medium provided by CobraPy\n",
    "    * bounds - dictionary - upper and lower bounds for the values the medium components are allowed to take,\n",
    "    determines the search space; if not provided defaults to 0, and current medium value\n",
    "    * costs - dictionary - the (monetary) cost of each component; if not provided defaults to unit costs\n",
    "    * opt_objective - string - indicates what is to be optimised\n",
    "    * biomass_objective - string - id of the biomass vector\n",
    "    * production_objective - string - id of the production reaction to be optmised\n",
    "    * n_start - integer - how many random media compositions are to be created to set up the BayesOpt\n",
    "    * n_iter - integer - how many candidate medium compositions should be found and evaluated\n",
    "    * n_candidates - integer - batch size; how many candidates to find per iteration\n",
    "    - AF_type - string - which acquisitin function to use\n",
    "\n",
    "    RETURNS:\n",
    "    A dictionary containing\n",
    "    * the acquisition function used\n",
    "    * the objective used (or combination of objectives)\n",
    "    * a dictionary with the upper and lower bounds of each medium components\n",
    "    * a dictionary with the cost of each medium component\n",
    "    * a list of all evaluated medium compositions\n",
    "    * a tensor with corresponding total medium costs\n",
    "    * a tensor with corresponding growth rates\n",
    "    * a binary tensor indicating if a  medium composition is on the pareto front\n",
    "    \"\"\"\n",
    "\n",
    "    # assert that the objective is one of the possibilities\n",
    "    opt_objective_types = ['growth-cost', 'growth-production', 'growth-production-cost',]\n",
    "    if opt_objective not in opt_objective_types:\n",
    "        raise ValueError(\"Invalid objective. Expected one of: %o\" % opt_objective_types)\n",
    "    \n",
    "    # assert that the acquisition function type is one of the possibilities\n",
    "    AF_types = ['qPAREGO', 'qEHVI', 'qNEHVI']\n",
    "    if AF_type not in AF_types:\n",
    "        raise ValueError(\"Invalid acquisition function. Expected one of: %o\" % AF_types)\n",
    "\n",
    "    # Set default values for medium, boundaries and costs\n",
    "    if medium is None:\n",
    "        medium = MetModel.medium  # Default medium to model.medium if not provided\n",
    "    if bounds is None:\n",
    "        # if no bounds are provided, set the lower limit to 0 and upper to value in medium\n",
    "        #bounds = {key: (0, 2*medium[key]+100) for key in medium.keys()} # used up until 09.10.2024\n",
    "        bounds = {key: (0, medium[key]) for key in medium.keys()}\n",
    "    if costs is None:\n",
    "        costs = {key: 1 for key in medium.keys()}  # Default unit cost if not provided\n",
    "    \n",
    "    # TODO: check that model.medium and costs have the same number of entries\n",
    "\n",
    "    # objective of metabolic model should always be to maximise growth\n",
    "    if biomass_objective is None:\n",
    "        raise ValueError(\"Please specifiy the biomass objective.\")\n",
    "    MetModel.objective_direction = 'max' \n",
    "    MetModel.objective = biomass_objective  \n",
    "\n",
    "    '''initialisation samples'''\n",
    "    # generate n_start initial data points (parameters and corresponding cost + growth rate)\n",
    "    initial_para, initial_growth, initial_production, initial_cost = generate_initial_data(\n",
    "        MetModel, medium, bounds, costs, n_samples = n_start,\n",
    "        opt_objective = objective,\n",
    "        biomass_objective = biomass_objective, \n",
    "        production_objective = production_objective\n",
    "        )\n",
    "   \n",
    "    # medium\n",
    "    medium_list = initial_para # list of dictonaries\n",
    "    medium_keys = medium_list[-1].keys() # extract keys from medium_list\n",
    "    # stack bounds; convert medium list to tensors and normalise it (0,1)\n",
    "    bounds_tensors_stacked, medium_tensors_normalised = convert_normalise_media(bounds, medium_list)\n",
    "    \n",
    "    '''reassign and normalise'''\n",
    "    # growth\n",
    "    growth_tensors = initial_growth # tensor\n",
    "    growth_tensors_normalised = normalise_1Dtensors(growth_tensors)\n",
    "    # cost\n",
    "    cost_tensors = initial_cost # tensor | <0 (generate_initial_data returns negative total costs)\n",
    "    # multiply by -1 to get pos. values, normalise, multiply with -1 again to get neg. values\n",
    "    cost_tensors_normalised = (normalise_1Dtensors(cost_tensors*(-1))*(-1))\n",
    "    # production\n",
    "    production_tensors = initial_production\n",
    "    production_tensors_normalised = torch.tensor([])\n",
    "    if production_objective != None:\n",
    "        production_tensors_normalised = normalise_1Dtensors(production_tensors)\n",
    "\n",
    "    is_pareto = torch.tensor([]) # empty\n",
    "\n",
    "    print(\"\\nINITIAL:\",\n",
    "          \"\\nGrowth:\", growth_tensors, growth_tensors_normalised,\n",
    "          \"\\nCosts:\", cost_tensors, cost_tensors_normalised,\n",
    "          \"\\nProduction\", production_tensors, production_tensors_normalised,\n",
    "          sep =\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # Optimise the trade-off between growth and cost\n",
    "    if opt_objective == \"growth-cost\":\n",
    "\n",
    "        # MAIN LOOP\n",
    "        for i in range(n_iter):\n",
    "            # Stack the list of tensors along a new dimension (dim=0) -> single tensor\n",
    "            medium_tensors_normalised_stacked = torch.stack(medium_tensors_normalised, dim = 0) # normalised\n",
    "            \n",
    "            # Use BayesOpt to change medium\n",
    "            # normalised growth and cost\n",
    "            candidates_tensor_normalised = find_next_candidates(\n",
    "                medium_tensors_normalised_stacked, \n",
    "                growth_tensors_normalised,\n",
    "                bounds_tensors_stacked,\n",
    "                n_candidates = n_candidates,\n",
    "                opt_objective = opt_objective,\n",
    "                cost_tensors = cost_tensors_normalised,\n",
    "                production_tensors = None,\n",
    "                AF_type = AF_type\n",
    "                )\n",
    "            \n",
    "            \"\"\"\n",
    "            # without normalisation of output (growth, cost)\n",
    "            candidates_tensor_normalised = find_next_candidates(\n",
    "                medium_tensors_normalised_stacked, \n",
    "                growth_tensors,\n",
    "                bounds_tensors_stacked,\n",
    "                n_candidates = n_candidates,\n",
    "                opt_objective = opt_objective,\n",
    "                cost_tensors = cost_tensors,\n",
    "                production_tensors = None,\n",
    "                AF_type = AF_type\n",
    "                )\n",
    "            \"\"\"\n",
    "            # evaluate each candidate medium using slim.optimize() - faster than optimize()\n",
    "            for j in range(n_candidates):\n",
    "                candidate_tensor_normalised = candidates_tensor_normalised[j]\n",
    "                # unnormlise new candidate\n",
    "                candidate_tensor_unnormalised = unnormalize(candidate_tensor_normalised, bounds_tensors_stacked)\n",
    "                # convert back to dictionary            \n",
    "                candidate_medium = convert_to_dict(candidate_tensor_unnormalised, medium_keys)\n",
    "                \n",
    "                \n",
    "                # for new medium compute new values\n",
    "                cost_tot = calc_cost_tot(costs, candidate_medium) # tensor\n",
    "\n",
    "                MetModel.medium = candidate_medium # assign new medium to model\n",
    "                growth = MetModel.slim_optimize() # calculate growth rate - float\n",
    "                # some model compositions lead to MetModel.slim_optimze returning NaN\n",
    "                # to avoid them from breaking the algorithm, set growth to zero\n",
    "                # same goes for negative growth which breaks normalisation\n",
    "                if (np.isnan(growth) or (growth < 0.0)):\n",
    "                    growth = 0\n",
    "\n",
    "                '''append to list/tensor and normalise resulting vector (min/max might have changed -> compute from scratch)'''\n",
    "                # medium lists\n",
    "                medium_list.append(candidate_medium)\n",
    "                medium_tensors_normalised.append(candidate_tensor_normalised)\n",
    "                # cost\n",
    "                cost_tot = -cost_tot  # BOtorch assumes maximisation, so we maximise the negative of the costs.\n",
    "                cost_tensors = torch.cat((cost_tensors, cost_tot), dim=0)  # Concatenate along dimension 0 (1D tensors)\n",
    "                cost_tensors_normalised = (normalise_1Dtensors(cost_tensors*(-1))*(-1))\n",
    "                # growth\n",
    "                growth_tensor = torch.tensor([growth], dtype=torch.double).to(**tkwargs)\n",
    "                growth_tensors = torch.cat((growth_tensors, growth_tensor), dim=0)  # Concatenate along dimension 0\n",
    "                growth_tensors_normalised = normalise_1Dtensors(growth_tensors)\n",
    "\n",
    "            print(\"Iteration:\\t\", i+1,\n",
    "                  \"\\nGrowth:\", growth_tensors, growth_tensors_normalised,\n",
    "                  \"\\nCosts:\", cost_tensors, cost_tensors_normalised,\n",
    "                  \"\\nProduction\", production_tensors, production_tensors_normalised,\n",
    "                  sep =\"\\n\")\n",
    "            \n",
    "            if ((i+1)%10 == 0):\n",
    "                print(\"Iteration:\\t\", i+1)\n",
    "\n",
    "\n",
    "        # Find all points on pareto front and return them     \n",
    "        # Stack the two objectives (growth rate and medium cost) into a single 2D tensor\n",
    "        # rows: candidates\n",
    "        # columns: growth rate (positive), medium costs (negative)\n",
    "        y = torch.stack((growth_tensors, cost_tensors), dim = 1)        \n",
    "        is_pareto = is_non_dominated(y.to(**tkwargs), maximize = True) # Compute non-dominated (Pareto front) points; i.e. optimal trade.offs\n",
    "    \n",
    "\n",
    "    # Optimise the trade-off between growth and production\n",
    "    elif opt_objective == \"growth-production\":  \n",
    "        if production_objective is None:\n",
    "            raise ValueError(\"Please specifiy the production objective.\")         \n",
    "\n",
    "        # MAIN LOOP\n",
    "        for i in range(n_iter):\n",
    "            # Stack the list of tensors along a new dimension (dim=0) -> single tensor\n",
    "            medium_tensors_normalised_stacked = torch.stack(medium_tensors_normalised, dim = 0) # normalised\n",
    "\n",
    "            # Use BayesOpt to change medium\n",
    "            # with  normalised output\n",
    "            candidates_tensor_normalised = find_next_candidates(\n",
    "                medium_tensors_normalised_stacked, \n",
    "                growth_tensors_normalised,\n",
    "                bounds_tensors_stacked,\n",
    "                n_candidates = n_candidates,\n",
    "                opt_objective = opt_objective,\n",
    "                cost_tensors = None, \n",
    "                production_tensors = production_tensors_normalised,\n",
    "                AF_type = AF_type\n",
    "                )\n",
    "            \"\"\"\n",
    "            # without normalisation of output (growth, production)\n",
    "            candidates_tensor_normalised = find_next_candidates(\n",
    "                medium_tensors_normalised_stacked, \n",
    "                growth_tensors,\n",
    "                bounds_tensors_stacked,\n",
    "                n_candidates = n_candidates,\n",
    "                opt_objective = opt_objective,\n",
    "                cost_tensors = None, \n",
    "                production_tensors = production_tensors,\n",
    "                AF_type = AF_type\n",
    "                )\n",
    "            \"\"\"\n",
    "            # evaluate each candidate medium\n",
    "            for j in range(n_candidates):\n",
    "                candidate_tensor_normalised = candidates_tensor_normalised[j]\n",
    "                # unnormlise new candidate\n",
    "                candidate_tensor_unnormalised = unnormalize(candidate_tensor_normalised, bounds_tensors_stacked)\n",
    "                # convert back to dictionary            \n",
    "                candidate_medium = convert_to_dict(candidate_tensor_unnormalised, medium_keys)\n",
    "                \n",
    "                # calculate the cost\n",
    "                cost_tot = calc_cost_tot(costs, candidate_medium) # tensor\n",
    "                \n",
    "                # for new medium compute new values\n",
    "                MetModel.medium = candidate_medium # assign candidate medium to model\n",
    "                \n",
    "                '''FBA'''\n",
    "                # assign biomass function as objective\n",
    "                MetModel.objective = biomass_objective\n",
    "                FBA_solution = MetModel.optimize()\n",
    "                growth = FBA_solution.fluxes[biomass_objective]\n",
    "                production = FBA_solution.fluxes[production_objective]\n",
    "                \n",
    "                # if either is NaN or smaller than zero, set to zero\n",
    "                # negative values cause problems for the scalarisation and are biologically implausible\n",
    "                if (np.isnan(growth) or (growth < 0.0)):\n",
    "                    growth = 0\n",
    "                if (np.isnan(production) or (production < 0.0)):\n",
    "                    production = 0  \n",
    "\n",
    "\n",
    "                '''append to new result to list/tensor'''\n",
    "                # medium lists\n",
    "                medium_list.append(candidate_medium)\n",
    "                medium_tensors_normalised.append(candidate_tensor_normalised)\n",
    "                # growth\n",
    "                growth_tensor = torch.tensor([growth], dtype=torch.double).to(**tkwargs)\n",
    "                growth_tensors = torch.cat((growth_tensors, growth_tensor), dim=0)  # Concatenate along dimension 0\n",
    "                growth_tensors_normalised = normalise_1Dtensors(growth_tensors)\n",
    "                # production\n",
    "                production_tensor = torch.tensor([production], dtype=torch.double).to(**tkwargs)\n",
    "                production_tensors = torch.cat((production_tensors, production_tensor), dim=0)  # Concatenate along dimension 0\n",
    "                production_tensors_normalised = normalise_1Dtensors(production_tensors)\n",
    "                # cost\n",
    "                cost_tot = -cost_tot  # BoTorch assumes maximisation, so we maximise the negative of the costs.\n",
    "                cost_tensors = torch.cat((cost_tensors, cost_tot), dim=0)  # Concatenate along dimension 0 (1D tensors)\n",
    "                cost_tensors_normalised = (normalise_1Dtensors(cost_tensors*(-1))*(-1))\n",
    "\n",
    "            if ((i+1)%10 == 0):\n",
    "                print(\"Iteration:\\t\", i+1)\n",
    "\n",
    "\n",
    "        # Find all points on pareto front and return them     \n",
    "        # Stack the two objectives (growth rate and production rate) into a single 2D tensor\n",
    "        # rows: candidates\n",
    "        # columns: growth rate (positive), production rate (positive)\n",
    "        y = torch.stack((growth_tensors, production_tensors), dim = 1)\n",
    "        is_pareto = is_non_dominated(y.to(**tkwargs), maximize = True) # Compute non-dominated (Pareto front) points; i.e. optimal trade.offs\n",
    "\n",
    "\n",
    "    # Optimise the trade-off between growth, production and cost\n",
    "    elif opt_objective == \"growth-production-cost\":\n",
    "        if production_objective is None:\n",
    "            raise ValueError(\"Please specifiy the production objective.\") \n",
    "        \n",
    "        # MAIN LOOP\n",
    "        for i in range(n_iter):\n",
    "            # Stack the list of tensors along a new dimension (dim=0) -> single tensor\n",
    "            medium_tensors_normalised_stacked = torch.stack(medium_tensors_normalised, dim = 0) # normalised\n",
    "\n",
    "\n",
    "            # Use BayesOpt to change medium\n",
    "            \"\"\"\n",
    "            candidates_tensor_normalised = find_next_candidates(\n",
    "                medium_tensors_normalised_stacked, \n",
    "                growth_tensors_normalised,\n",
    "                bounds_tensors_stacked,\n",
    "                n_candidates = n_candidates,\n",
    "                opt_objective = opt_objective,\n",
    "                cost_tensors = cost_tensors_normalised, \n",
    "                production_tensors = production_tensors_normalised,\n",
    "                AF_type = AF_type\n",
    "                )\n",
    "            \"\"\"\n",
    "            # without normalisation of output (growth, production, cost)\n",
    "            candidates_tensor_normalised = find_next_candidates(\n",
    "                medium_tensors_normalised_stacked, \n",
    "                growth_tensors,\n",
    "                bounds_tensors_stacked,\n",
    "                n_candidates = n_candidates,\n",
    "                opt_objective = opt_objective,\n",
    "                cost_tensors = cost_tensors, \n",
    "                production_tensors = production_tensors,\n",
    "                AF_type = AF_type\n",
    "                )\n",
    "            \n",
    "            # evaluate each candidate medium\n",
    "            for j in range(n_candidates):\n",
    "                candidate_tensor_normalised = candidates_tensor_normalised[j]\n",
    "                # unnormlise new candidate\n",
    "                candidate_tensor_unnormalised = unnormalize(candidate_tensor_normalised, bounds_tensors_stacked)\n",
    "                # convert back to dictionary            \n",
    "                candidate_medium = convert_to_dict(candidate_tensor_unnormalised, medium_keys)\n",
    "                \n",
    "                # calculate the cost\n",
    "                cost_tot = calc_cost_tot(costs, candidate_medium) # tensor\n",
    "                \n",
    "                # for new medium compute new values\n",
    "                MetModel.medium = candidate_medium # assign candidate medium to model\n",
    "    \n",
    "                \n",
    "                '''FBA'''\n",
    "                # assign biomass function as objective\n",
    "                MetModel.objective = biomass_objective\n",
    "                FBA_solution = MetModel.optimize()\n",
    "                growth = FBA_solution.fluxes[biomass_objective]\n",
    "                production = FBA_solution.fluxes[production_objective]\n",
    "                \n",
    "                # if either is NaN or smaller than zero, set to zero\n",
    "                # negative values cause problems for the scalarisation and are biologically implausible\n",
    "                if (np.isnan(growth) or (growth < 0.0)):\n",
    "                    growth = 0\n",
    "                if (np.isnan(production) or (production < 0.0)):\n",
    "                    production = 0 \n",
    "\n",
    "\n",
    "                '''append to new result to list/tensor'''\n",
    "                # medium lists\n",
    "                medium_list.append(candidate_medium)\n",
    "                medium_tensors_normalised.append(candidate_tensor_normalised)\n",
    "                # growth\n",
    "                growth_tensor = torch.tensor([growth], dtype=torch.double).to(**tkwargs)\n",
    "                growth_tensors = torch.cat((growth_tensors, growth_tensor), dim=0)  # Concatenate along dimension 0\n",
    "                growth_tensors_normalised = normalise_1Dtensors(growth_tensors)\n",
    "                # production\n",
    "                production_tensor = torch.tensor([production], dtype=torch.double).to(**tkwargs)\n",
    "                production_tensors = torch.cat((production_tensors, production_tensor), dim=0)  # Concatenate along dimension 0\n",
    "                production_tensors_normalised = normalise_1Dtensors(production_tensors)\n",
    "                # cost\n",
    "                cost_tot = -cost_tot  # BoTorch assumes maximisation, so we maximise the negative of the costs.\n",
    "                cost_tensors = torch.cat((cost_tensors, cost_tot), dim=0)  # Concatenate along dimension 0 (1D tensors)\n",
    "                cost_tensors_normalised = (normalise_1Dtensors(cost_tensors*(-1))*(-1))\n",
    "\n",
    "            if ((i+1)%10 == 0):\n",
    "                print(\"Iteration:\\t\", i+1)\n",
    "\n",
    "\n",
    "        # Find all points on pareto front and return them     \n",
    "        # Stack the three objectives (growth rate, production rate and cost) into a single 2D tensor\n",
    "        # rows: candidates\n",
    "        # columns: growth rate (positive), production rate (positive), cost (negative)\n",
    "        y = torch.stack((growth_tensors, production_tensors, cost_tensors), dim = 1)   \n",
    "        is_pareto = is_non_dominated(y.to(**tkwargs), maximize = True) # Compute non-dominated (Pareto front) points; i.e. optimal trade.offs\n",
    "    \n",
    "\n",
    "\n",
    "     # multiply cost_tensor with -1 so that it returns positive values\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"acquisition function\" : AF_type,\n",
    "        \"objective\" : opt_objective,\n",
    "        \"medium component bounds\" : bounds, \n",
    "        \"medium component costs\" : costs,\n",
    "        \"medium list\" : medium_list, \n",
    "        \"cost tensors\" : (cost_tensors*(-1)),\n",
    "        \"growth rate tensors\" : growth_tensors,\n",
    "        \"production tensors\" : production_tensors,\n",
    "        \"is pareto\" : is_pareto,\n",
    "        \"biomass_objective\" : biomass_objective,\n",
    "        \"production_objective\" : production_objective\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "374ed588-3595-4c5a-b344-c1e2d36fb6b3"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Bayesian-opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
